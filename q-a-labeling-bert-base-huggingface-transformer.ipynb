{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-base TensorFlow 2.0\n",
    "\n",
    "This kernel does not explore the data. For that you could check out some of the great EDA kernels: [introduction](https://www.kaggle.com/corochann/google-quest-first-data-introduction), [getting started](https://www.kaggle.com/phoenix9032/get-started-with-your-questions-eda-model-nn) & [another getting started](https://www.kaggle.com/hamditarek/get-started-with-nlp-lda-lsa). This kernel is an example of a TensorFlow 2.0 Bert-base implementation, using ~~TensorFow Hub~~ Huggingface transformer. <br><br>\n",
    "\n",
    "---\n",
    "**Update 1 (Commit 7):**\n",
    "* removing penultimate dense layer; now there's only one dense layer (output layer) for fine-tuning\n",
    "* using BERT's sequence_output instead of pooled_output as input for the dense layer\n",
    "---\n",
    "\n",
    "**Update 2 (Commit 8):**\n",
    "* adjusting `_trim_input()` --- now have a q_max_len and a_max_len, instead of 'keeping the ratio the same' while trimming.\n",
    "* **importantly:** now also includes question_title for the input sequence\n",
    "---\n",
    "\n",
    "**Update 3 (Commit 9)**\n",
    "<br><br>*A lot of experiments can be made with the title + body + answer sequence. Feel free to look into e.g. (1) inventing new tokens (add it to '../input/path-to-bert-folder/assets/vocab.txt'), (2) keeping \\[SEP\\] between title and body but modify `_get_segments()`, (3) using the \\[PAD\\] token, or (4) merging title and body without any kind of separation. In this commit I'm doing (2). I also tried (3) offline, and they both perform better than in commit 8, in terms of validation rho.*<br>\n",
    "\n",
    "* ignoring first \\[SEP\\] token in `_get_segments()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Update 4 (Commit 11)**\n",
    "* **Now using Huggingface transformer instead of TFHub** (note major changes in the code). This creates the possibility to easily try out different architectures like XLNet, Roberta etc. As well as easily outputting the hidden states of the transformer.\n",
    "* two separate inputs (title+body and answer) for BERT\n",
    "* removed snapshot average (now only using last (third) epoch). This will likely decrease performance, but it's not feasible to use ~ 5 x 4 models for a single bert prediction in practice. \n",
    "* only training for 2 epochs instead of 3 (to manage 2h limit)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../input/sacremoses > /dev/null\n",
    "#!pip install git+https://github.com/huggingface/transformers\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../input/transformers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read data and tokenizer\n",
    "\n",
    "Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlnet-data/xlnet-base-cased-spiece.model\n",
      "/kaggle/input/xlnet-data/xlnet-base-cased-tf_model.h5\n",
      "/kaggle/input/roberta-data/roberta-base-tf_model.h5\n",
      "/kaggle/input/roberta-data/roberta-base-vocab.json\n",
      "/kaggle/input/roberta-data/roberta-base-merges.txt\n",
      "/kaggle/input/google-quest-challenge/test.csv\n",
      "/kaggle/input/google-quest-challenge/sample_submission.csv\n",
      "/kaggle/input/google-quest-challenge/train.csv\n",
      "/kaggle/input/sacremoses/CONTRIBUTORS.md\n",
      "/kaggle/input/sacremoses/README.md\n",
      "/kaggle/input/sacremoses/requirements.txt\n",
      "/kaggle/input/sacremoses/setup.py\n",
      "/kaggle/input/sacremoses/sacremoses/tokenize.py\n",
      "/kaggle/input/sacremoses/sacremoses/subwords.py\n",
      "/kaggle/input/sacremoses/sacremoses/util.py\n",
      "/kaggle/input/sacremoses/sacremoses/chinese.py\n",
      "/kaggle/input/sacremoses/sacremoses/normalize.py\n",
      "/kaggle/input/sacremoses/sacremoses/cli.py\n",
      "/kaggle/input/sacremoses/sacremoses/__init__.py\n",
      "/kaggle/input/sacremoses/sacremoses/truecase.py\n",
      "/kaggle/input/sacremoses/sacremoses/corpus.py\n",
      "/kaggle/input/sacremoses/sacremoses/test/test_truecaser.py\n",
      "/kaggle/input/sacremoses/sacremoses/test/test_normalizer.py\n",
      "/kaggle/input/sacremoses/sacremoses/test/test_corpus.py\n",
      "/kaggle/input/sacremoses/sacremoses/test/test_tokenizer.py\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.pt\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.lv\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.cs\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ru\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.pl\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ca\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.en\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ro\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.zh\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.es\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/README.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ga\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sk\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.it\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.yue\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.el\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ta\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.hu\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.is\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.fr\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.nl\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.de\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sl\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sv\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.fi\n",
      "/kaggle/input/sacremoses/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.lt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsAlnum-unichars-au.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Number.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/CJK.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Currency_Symbol.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Han.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Separator.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsSc.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Open_Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Line_Separator.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/CJKSymbols.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsSo.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsAlnum.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Titlecase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsN.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Hangul_Syllables.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Katakana.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Hiragana.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Close_Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Hangul.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsAlpha-unichars-au.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsPi.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Symbol.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Lowercase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsUpper.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsLower.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/Uppercase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsAlpha.txt\n",
      "/kaggle/input/sacremoses/sacremoses/data/perluniprops/IsPf.txt\n",
      "/kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-vocab.txt\n",
      "/kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-tf_model.h5\n",
      "/kaggle/input/transformers/CONTRIBUTING.md\n",
      "/kaggle/input/transformers/valohai.yaml\n",
      "/kaggle/input/transformers/requirements-dev.txt\n",
      "/kaggle/input/transformers/transformers-cli\n",
      "/kaggle/input/transformers/LICENSE\n",
      "/kaggle/input/transformers/README.md\n",
      "/kaggle/input/transformers/deploy_multi_version_doc.sh\n",
      "/kaggle/input/transformers/requirements.txt\n",
      "/kaggle/input/transformers/MANIFEST.in\n",
      "/kaggle/input/transformers/setup.py\n",
      "/kaggle/input/transformers/hubconf.py\n",
      "/kaggle/input/transformers/utils/download_glue_data.py\n",
      "/kaggle/input/transformers/docs/README.md\n",
      "/kaggle/input/transformers/docs/requirements.txt\n",
      "/kaggle/input/transformers/docs/Makefile\n",
      "/kaggle/input/transformers/docs/source/torchscript.rst\n",
      "/kaggle/input/transformers/docs/source/migration.md\n",
      "/kaggle/input/transformers/docs/source/multilingual.rst\n",
      "/kaggle/input/transformers/docs/source/converting_tensorflow_models.rst\n",
      "/kaggle/input/transformers/docs/source/installation.md\n",
      "/kaggle/input/transformers/docs/source/benchmarks.md\n",
      "/kaggle/input/transformers/docs/source/serialization.rst\n",
      "/kaggle/input/transformers/docs/source/conf.py\n",
      "/kaggle/input/transformers/docs/source/examples.md\n",
      "/kaggle/input/transformers/docs/source/index.rst\n",
      "/kaggle/input/transformers/docs/source/pretrained_models.rst\n",
      "/kaggle/input/transformers/docs/source/notebooks.rst\n",
      "/kaggle/input/transformers/docs/source/quickstart.md\n",
      "/kaggle/input/transformers/docs/source/bertology.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/gpt2.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/distilbert.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/xlm.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/transformerxl.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/xlnet.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/bert.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/ctrl.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/camembert.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/gpt.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/roberta.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/albert.rst\n",
      "/kaggle/input/transformers/docs/source/model_doc/auto.rst\n",
      "/kaggle/input/transformers/docs/source/_static/css/huggingface.css\n",
      "/kaggle/input/transformers/docs/source/_static/css/code-snippets.css\n",
      "/kaggle/input/transformers/docs/source/_static/css/Calibre-Thin.otf\n",
      "/kaggle/input/transformers/docs/source/_static/css/Calibre-Medium.otf\n",
      "/kaggle/input/transformers/docs/source/_static/css/Calibre-Regular.otf\n",
      "/kaggle/input/transformers/docs/source/_static/css/Calibre-Light.ttf\n",
      "/kaggle/input/transformers/docs/source/_static/js/huggingface_logo.svg\n",
      "/kaggle/input/transformers/docs/source/_static/js/custom.js\n",
      "/kaggle/input/transformers/docs/source/imgs/warmup_cosine_schedule.png\n",
      "/kaggle/input/transformers/docs/source/imgs/warmup_cosine_warm_restarts_schedule.png\n",
      "/kaggle/input/transformers/docs/source/imgs/warmup_linear_schedule.png\n",
      "/kaggle/input/transformers/docs/source/imgs/transformers_logo_name.png\n",
      "/kaggle/input/transformers/docs/source/imgs/warmup_cosine_hard_restarts_schedule.png\n",
      "/kaggle/input/transformers/docs/source/imgs/warmup_constant_schedule.png\n",
      "/kaggle/input/transformers/docs/source/main_classes/optimizer_schedules.rst\n",
      "/kaggle/input/transformers/docs/source/main_classes/tokenizer.rst\n",
      "/kaggle/input/transformers/docs/source/main_classes/processors.rst\n",
      "/kaggle/input/transformers/docs/source/main_classes/model.rst\n",
      "/kaggle/input/transformers/docs/source/main_classes/configuration.rst\n",
      "/kaggle/input/transformers/templates/adding_a_new_example_script/utils_xxx.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_example_script/README.md\n",
      "/kaggle/input/transformers/templates/adding_a_new_example_script/run_xxx.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/README.md\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/modeling_tf_xxx.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/tokenization_xxx.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/modeling_xxx.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/configuration_xxx.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/tests/tokenization_xxx_test.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/tests/modeling_xxx_test.py\n",
      "/kaggle/input/transformers/templates/adding_a_new_model/tests/modeling_tf_xxx_test.py\n",
      "/kaggle/input/transformers/notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb\n",
      "/kaggle/input/transformers/notebooks/Comparing-TF-and-PT-models.ipynb\n",
      "/kaggle/input/transformers/notebooks/Comparing-TF-and-PT-models-SQuAD.ipynb\n",
      "/kaggle/input/transformers/notebooks/Comparing-PT-and-TF-models.ipynb\n",
      "/kaggle/input/transformers/docker/Dockerfile\n",
      "/kaggle/input/transformers/transformers/tokenization_utils.py\n",
      "/kaggle/input/transformers/transformers/modeling_albert.py\n",
      "/kaggle/input/transformers/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers/modeling_xlm.py\n",
      "/kaggle/input/transformers/transformers/modeling_camembert.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_auto.py\n",
      "/kaggle/input/transformers/transformers/modeling_distilbert.py\n",
      "/kaggle/input/transformers/transformers/modeling_roberta.py\n",
      "/kaggle/input/transformers/transformers/tokenization_xlnet.py\n",
      "/kaggle/input/transformers/transformers/configuration_utils.py\n",
      "/kaggle/input/transformers/transformers/modeling_beam_search.py\n",
      "/kaggle/input/transformers/transformers/tokenization_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers/tokenization_xlm.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_utils.py\n",
      "/kaggle/input/transformers/transformers/modeling_gpt2.py\n",
      "/kaggle/input/transformers/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_bert.py\n",
      "/kaggle/input/transformers/transformers/modeling_transfo_xl_utilities.py\n",
      "/kaggle/input/transformers/transformers/configuration_xlm.py\n",
      "/kaggle/input/transformers/transformers/configuration_xlnet.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_ctrl.py\n",
      "/kaggle/input/transformers/transformers/tokenization_ctrl.py\n",
      "/kaggle/input/transformers/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_xlm.py\n",
      "/kaggle/input/transformers/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers/configuration_bert.py\n",
      "/kaggle/input/transformers/transformers/configuration_ctrl.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_gpt2.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_distilbert.py\n",
      "/kaggle/input/transformers/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py\n",
      "/kaggle/input/transformers/transformers/configuration_openai.py\n",
      "/kaggle/input/transformers/transformers/modeling_encoder_decoder.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_openai.py\n",
      "/kaggle/input/transformers/transformers/configuration_roberta.py\n",
      "/kaggle/input/transformers/transformers/convert_pytorch_checkpoint_to_tf2.py\n",
      "/kaggle/input/transformers/transformers/configuration_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers/modeling_openai.py\n",
      "/kaggle/input/transformers/transformers/configuration_distilbert.py\n",
      "/kaggle/input/transformers/transformers/modeling_utils.py\n",
      "/kaggle/input/transformers/transformers/tokenization_openai.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_roberta.py\n",
      "/kaggle/input/transformers/transformers/hf_api.py\n",
      "/kaggle/input/transformers/transformers/modeling_ctrl.py\n",
      "/kaggle/input/transformers/transformers/optimization.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_albert.py\n",
      "/kaggle/input/transformers/transformers/configuration_auto.py\n",
      "/kaggle/input/transformers/transformers/configuration_camembert.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_transfo_xl_utilities.py\n",
      "/kaggle/input/transformers/transformers/tokenization_auto.py\n",
      "/kaggle/input/transformers/transformers/tokenization_albert.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_xlnet.py\n",
      "/kaggle/input/transformers/transformers/modeling_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers/tokenization_bert.py\n",
      "/kaggle/input/transformers/transformers/__init__.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_pytorch_utils.py\n",
      "/kaggle/input/transformers/transformers/tokenization_distilbert.py\n",
      "/kaggle/input/transformers/transformers/tokenization_gpt2.py\n",
      "/kaggle/input/transformers/transformers/tokenization_camembert.py\n",
      "/kaggle/input/transformers/transformers/modeling_tf_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers/__main__.py\n",
      "/kaggle/input/transformers/transformers/configuration_gpt2.py\n",
      "/kaggle/input/transformers/transformers/tokenization_roberta.py\n",
      "/kaggle/input/transformers/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers/configuration_albert.py\n",
      "/kaggle/input/transformers/transformers/modeling_bert.py\n",
      "/kaggle/input/transformers/transformers/optimization_tf.py\n",
      "/kaggle/input/transformers/transformers/file_utils.py\n",
      "/kaggle/input/transformers/transformers/modeling_auto.py\n",
      "/kaggle/input/transformers/transformers/modeling_xlnet.py\n",
      "/kaggle/input/transformers/transformers/commands/__init__.py\n",
      "/kaggle/input/transformers/transformers/commands/user.py\n",
      "/kaggle/input/transformers/transformers/data/__init__.py\n",
      "/kaggle/input/transformers/transformers/data/processors/glue.py\n",
      "/kaggle/input/transformers/transformers/data/processors/__init__.py\n",
      "/kaggle/input/transformers/transformers/data/processors/utils.py\n",
      "/kaggle/input/transformers/transformers/data/processors/xnli.py\n",
      "/kaggle/input/transformers/transformers/data/metrics/__init__.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_auto_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_openai_test.py\n",
      "/kaggle/input/transformers/transformers/tests/optimization_tf_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_tests_commons.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_bert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_albert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_openai_gpt_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_xlm_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_auto_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_encoder_decoder_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers/tests/hf_api_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_albert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_xlm_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_roberta_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_common_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_utils_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers/tests/__init__.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_roberta_test.py\n",
      "/kaggle/input/transformers/transformers/tests/utils.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_roberta_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_common_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_auto_test.py\n",
      "/kaggle/input/transformers/transformers/tests/configuration_common_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_xlm_test.py\n",
      "/kaggle/input/transformers/transformers/tests/optimization_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_bert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_openai_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers/tests/modeling_tf_bert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/tokenization_albert_test.py\n",
      "/kaggle/input/transformers/transformers/tests/fixtures/sample_text.txt\n",
      "/kaggle/input/transformers/transformers/tests/fixtures/input.txt\n",
      "/kaggle/input/transformers/transformers/tests/fixtures/test_sentencepiece.model\n",
      "/kaggle/input/transformers/transformers/tests/fixtures/spiece.model\n",
      "/kaggle/input/transformers/examples/utils_summarization_test.py\n",
      "/kaggle/input/transformers/examples/run_glue.py\n",
      "/kaggle/input/transformers/examples/run_squad.py\n",
      "/kaggle/input/transformers/examples/utils_squad_evaluate.py\n",
      "/kaggle/input/transformers/examples/run_multiple_choice.py\n",
      "/kaggle/input/transformers/examples/utils_summarization.py\n",
      "/kaggle/input/transformers/examples/README.md\n",
      "/kaggle/input/transformers/examples/run_lm_finetuning.py\n",
      "/kaggle/input/transformers/examples/utils_squad.py\n",
      "/kaggle/input/transformers/examples/run_tf_glue.py\n",
      "/kaggle/input/transformers/examples/requirements.txt\n",
      "/kaggle/input/transformers/examples/run_bertology.py\n",
      "/kaggle/input/transformers/examples/run_generation.py\n",
      "/kaggle/input/transformers/examples/run_xnli.py\n",
      "/kaggle/input/transformers/examples/run_tf_ner.py\n",
      "/kaggle/input/transformers/examples/utils_multiple_choice.py\n",
      "/kaggle/input/transformers/examples/test_examples.py\n",
      "/kaggle/input/transformers/examples/run_summarization_finetuning.py\n",
      "/kaggle/input/transformers/examples/run_ner.py\n",
      "/kaggle/input/transformers/examples/benchmarks.py\n",
      "/kaggle/input/transformers/examples/utils_ner.py\n",
      "/kaggle/input/transformers/examples/distillation/README.md\n",
      "/kaggle/input/transformers/examples/distillation/requirements.txt\n",
      "/kaggle/input/transformers/examples/distillation/distiller.py\n",
      "/kaggle/input/transformers/examples/distillation/run_squad_w_distillation.py\n",
      "/kaggle/input/transformers/examples/distillation/train.py\n",
      "/kaggle/input/transformers/examples/distillation/grouped_batch_sampler.py\n",
      "/kaggle/input/transformers/examples/distillation/lm_seqs_dataset.py\n",
      "/kaggle/input/transformers/examples/distillation/utils.py\n",
      "/kaggle/input/transformers/examples/distillation/training_configs/distilgpt2.json\n",
      "/kaggle/input/transformers/examples/distillation/training_configs/distilbert-base-uncased.json\n",
      "/kaggle/input/transformers/examples/distillation/scripts/token_counts.py\n",
      "/kaggle/input/transformers/examples/distillation/scripts/extract.py\n",
      "/kaggle/input/transformers/examples/distillation/scripts/binarized_data.py\n",
      "/kaggle/input/transformers/examples/distillation/scripts/extract_distilbert.py\n",
      "/kaggle/input/transformers/examples/tests_samples/.gitignore\n",
      "/kaggle/input/transformers/examples/tests_samples/MRPC/dev.tsv\n",
      "/kaggle/input/transformers/examples/tests_samples/MRPC/train.tsv\n",
      "/kaggle/input/transformers/examples/tests_samples/SQUAD/dev-v2.0-small.json\n",
      "/kaggle/input/transformers/examples/contrib/run_transfo_xl.py\n",
      "/kaggle/input/transformers/examples/contrib/README.md\n",
      "/kaggle/input/transformers/examples/contrib/run_openai_gpt.py\n",
      "/kaggle/input/transformers/examples/contrib/run_swag.py\n",
      "/kaggle/input/transformers/examples/contrib/run_camembert.py\n",
      "/kaggle/input/transformers/examples/pplm/run_pplm_discrim_train.py\n",
      "/kaggle/input/transformers/examples/pplm/README.md\n",
      "/kaggle/input/transformers/examples/pplm/pplm_classification_head.py\n",
      "/kaggle/input/transformers/examples/pplm/run_pplm.py\n",
      "/kaggle/input/transformers/examples/pplm/imgs/headfigure.png\n",
      "/kaggle/input/transformers/examples/pplm/imgs/wooly.png\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (6079, 41)\n",
      "test shape = (476, 11)\n",
      "\n",
      "output categories:\n",
      "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "input categories:\n",
      "\t ['question_title', 'question_body', 'answer']\n"
     ]
    }
   ],
   "source": [
    "PATH = '../input/google-quest-challenge/'\n",
    "\n",
    "# BERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\n",
    "# tokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n",
    "\n",
    "BERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\n",
    "RoBERTa_PATH = '../input/roberta-data/'\n",
    "XLNet_PATH = '../input/xlnet-data/'\n",
    "#tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(vocab_file=RoBERTa_PATH+'roberta-base-vocab.json', merges_file=RoBERTa_PATH+'roberta-base-merges.txt',\n",
    "                                             pretrained_model_name_or_path=RoBERTa_PATH+'roberta-base-tf_model.h5')\n",
    "#tokenizer = XLNetTokenizer.from_pretrained(XLNet_PATH+'xlnet-base-cased-spiece.model')\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 384\n",
    "\n",
    "df_train = pd.read_csv(PATH+'train.csv')\n",
    "df_test = pd.read_csv(PATH+'test.csv')\n",
    "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)\n",
    "\n",
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])\n",
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing functions\n",
    "\n",
    "These are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n",
    "\n",
    "*update 4:* credits to [Minh](https://www.kaggle.com/dathudeptrai) for this implementation. If I'm not mistaken, it could be used directly with other Huggingface transformers too! Note that due to the 2 x 512 input, it will require significantly more memory when finetuning BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation_strategy=truncation_strategy)\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title + ' ' + question, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        answer, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create model\n",
    "\n",
    "`compute_spearmanr()` is used to compute the competition metric for the validation set\n",
    "<br><br>\n",
    "`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def compute_spearmanr(trues, preds):\n",
    "    rhos = []\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        rhos.append(\n",
    "            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
    "    return np.mean(rhos)\n",
    "\n",
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    # Version of attention mask for XLNet\n",
    "    #q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.float32)\n",
    "    #a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.float32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    #config = BertConfig() # print(config) to see settings\n",
    "    config = RobertaConfig()\n",
    "    # Version for RoBERTa\n",
    "    config.vocab_size = 50265\n",
    "    config.max_position_embeddings = 514\n",
    "    config.type_vocab_size = 1\n",
    "    #config = XLNetConfig()\n",
    "    # Version of base XLNetConfig default is large version\n",
    "    #config.d_inner = 3072\n",
    "    #config.n_head = 12\n",
    "    #config.d_model = 768\n",
    "    #config.n_layer = 12\n",
    "    print(config)\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "    #bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    \n",
    "    #bert_model = TFBertModel(config=config)\n",
    "    roberta_model = TFRobertaModel.from_pretrained(RoBERTa_PATH+'roberta-base-tf_model.h5', config=config)\n",
    "    #xlnet_model = TFXLNetModel.from_pretrained(XLNet_PATH+'xlnet-base-cased-tf_model.h5', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    # Bert version of embeddings\n",
    "    #q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    #a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "    # XLNet version of embeddings\n",
    "    #q_embedding = xlnet_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    #a_embedding = xlnet_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "    # RoBERTa version of embeddings\n",
    "    q_embedding = roberta_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = roberta_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n",
    "\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "        self.test_inputs = test_data\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "        self.test_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "        \n",
    "        rho_val = compute_spearmanr(\n",
    "            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n",
    "        \n",
    "        print(\"\\nvalidation rho: %.4f\" % rho_val)\n",
    "        \n",
    "        if self.fold is not None:\n",
    "            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n",
    "        \n",
    "        self.test_predictions.append(\n",
    "            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n",
    "        )\n",
    "\n",
    "def train_and_predict(model, train_data, valid_data, test_data, \n",
    "                      learning_rate, epochs, batch_size, loss_function, fold):\n",
    "       \n",
    "    custom_callback = CustomCallback(\n",
    "        valid_data=(valid_data[0], valid_data[1]), \n",
    "        test_data=test_data,\n",
    "        batch_size=batch_size,\n",
    "        fold=None)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    model.fit(train_data[0], train_data[1], epochs=epochs, \n",
    "              batch_size=batch_size, callbacks=[custom_callback])\n",
    "    \n",
    "    return custom_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Obtain inputs and targets, as well as the indices of the train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62520101596d43aa86154c06ab1ee018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc42c6e7faa448178814480d322f62d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Training, validation and testing\n",
    "\n",
    "Loops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Train on 4863 samples\n",
      "Epoch 1/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3934\n",
      "validation rho: 0.3649\n",
      "4863/4863 [==============================] - 576s 119ms/sample - loss: 0.3934\n",
      "Epoch 2/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3660\n",
      "validation rho: 0.3909\n",
      "4863/4863 [==============================] - 535s 110ms/sample - loss: 0.3660\n",
      "Epoch 3/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3552\n",
      "validation rho: 0.3997\n",
      "4863/4863 [==============================] - 535s 110ms/sample - loss: 0.3553\n",
      "Epoch 4/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3468\n",
      "validation rho: 0.4041\n",
      "4863/4863 [==============================] - 535s 110ms/sample - loss: 0.3467\n",
      "Epoch 5/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3371\n",
      "validation rho: 0.4056\n",
      "4863/4863 [==============================] - 535s 110ms/sample - loss: 0.3372\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Train on 4863 samples\n",
      "Epoch 1/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3945\n",
      "validation rho: 0.3755\n",
      "4863/4863 [==============================] - 572s 118ms/sample - loss: 0.3944\n",
      "Epoch 2/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3683\n",
      "validation rho: 0.3991\n",
      "4863/4863 [==============================] - 534s 110ms/sample - loss: 0.3682\n",
      "Epoch 3/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3567\n",
      "validation rho: 0.4070\n",
      "4863/4863 [==============================] - 534s 110ms/sample - loss: 0.3567\n",
      "Epoch 4/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3473\n",
      "validation rho: 0.4128\n",
      "4863/4863 [==============================] - 534s 110ms/sample - loss: 0.3473\n",
      "Epoch 5/5\n",
      "4860/4863 [============================>.] - ETA: 0s - loss: 0.3384\n",
      "validation rho: 0.4146\n",
      "4863/4863 [==============================] - 534s 110ms/sample - loss: 0.3385\n"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n",
    "\n",
    "valid_preds = []\n",
    "test_preds = []\n",
    "histories = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "    # will actually only do 2 folds (out of 5) to manage < 2h\n",
    "    if fold in [0, 2]:\n",
    "    # XLNet version only do 1 folds (out of 5) to manage < 2h\n",
    "    #if fold == 0:\n",
    "    #if fold < 3:\n",
    "\n",
    "        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "        train_outputs = outputs[train_idx]\n",
    "\n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "        \n",
    "        K.clear_session()\n",
    "        model = create_model()\n",
    "        #model.load_weights(RoBERTa_PATH+'roberta-base-tf_model.h5', by_name=True)\n",
    "        # Version without history\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "        #model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        #model.fit(train_inputs, train_outputs, epochs=5, batch_size=6)\n",
    "        # model.save_weights(f'bert-{fold}.h5')\n",
    "        #valid_preds.append(model.predict(valid_inputs))\n",
    "        #test_preds.append(model.predict(test_inputs))\n",
    "        \n",
    "        #rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
    "        #print('validation score = ', rho_val)\n",
    "        \n",
    "        # history contains two lists of valid and test preds respectively:\n",
    "        #  [valid_predictions_{fold}, test_predictions_{fold}]\n",
    "        history = train_and_predict(model, \n",
    "                          train_data=(train_inputs, train_outputs), \n",
    "                          valid_data=(valid_inputs, valid_outputs),\n",
    "                          test_data=test_inputs, \n",
    "                          learning_rate=3e-5, epochs=5, batch_size=6,\n",
    "                          loss_function='binary_crossentropy', fold=fold)\n",
    "\n",
    "        histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Process and submit test predictions\n",
    "\n",
    "Average fold predictions, then save as `submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sub.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n",
    "\n",
    "test_predictions = [histories[i].test_predictions for i in range(len(histories))]\n",
    "test_predictions = [np.average(test_predictions[i], axis=0, weights=[1./18, 1./6, 2./9, 2./9, 1./3]) for i in range(len(test_predictions))]\n",
    "test_predictions = np.mean(test_predictions, axis=0)\n",
    "\n",
    "df_sub.iloc[:, 1:] = test_predictions\n",
    "\n",
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1478d38ea6644765a59508929fd5d052": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1dd5f8a1541e493399d05c3c9da48e48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fe79049ec2a84cbe9874b7ce6e98c019",
       "placeholder": "",
       "style": "IPY_MODEL_65ed130c69f34fafa86eb29771cd721d",
       "value": " 6079/? [01:02&lt;00:00, 96.92it/s]"
      }
     },
     "304b9140df5544098509a5fd365a9dec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3fdd35ca30f44dd1ad3ea5e38d1dc90f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62520101596d43aa86154c06ab1ee018": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d4534fe698b54eb18ca5a56c14b34cde",
        "IPY_MODEL_1dd5f8a1541e493399d05c3c9da48e48"
       ],
       "layout": "IPY_MODEL_304b9140df5544098509a5fd365a9dec"
      }
     },
     "65ed130c69f34fafa86eb29771cd721d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6f227f1b33fc410a83c3bb8d7d815bac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8216f1a8e83f4cf0ab992d5eee3b4594",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96459e7a87d644318548d74c9a0f506c",
       "value": 1.0
      }
     },
     "772665ff15ff4e2facc963273126a988": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7fd594cb9b864bb7a5193eb3343c00b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3fdd35ca30f44dd1ad3ea5e38d1dc90f",
       "placeholder": "",
       "style": "IPY_MODEL_b57f02129d5f473e9751bfa1c3bd5daf",
       "value": " 476/? [00:04&lt;00:00, 108.51it/s]"
      }
     },
     "8216f1a8e83f4cf0ab992d5eee3b4594": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96459e7a87d644318548d74c9a0f506c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b57f02129d5f473e9751bfa1c3bd5daf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d4534fe698b54eb18ca5a56c14b34cde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_772665ff15ff4e2facc963273126a988",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fde68610726243478cb30944b118bf02",
       "value": 1.0
      }
     },
     "fc42c6e7faa448178814480d322f62d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6f227f1b33fc410a83c3bb8d7d815bac",
        "IPY_MODEL_7fd594cb9b864bb7a5193eb3343c00b9"
       ],
       "layout": "IPY_MODEL_1478d38ea6644765a59508929fd5d052"
      }
     },
     "fde68610726243478cb30944b118bf02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "fe79049ec2a84cbe9874b7ce6e98c019": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
